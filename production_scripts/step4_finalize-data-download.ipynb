{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "515b9b0e-f04c-4027-bf17-2a05fb3a3230",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Step 4: Finalize data for downloan\n",
    "\n",
    "This script brings the annual PFAF baseline and future indicators into clean tables that contain the raw, score, category, and label.\n",
    "\n",
    "The baseline table will use the same naming mechanism as Aq 3.0\n",
    "The future table will use this format: \n",
    "  bau30_ws_x_r -> SSP 3 7.0 for 2030 water stress raw value\n",
    "\n",
    "The indicators included as of now are: water stress (ws), water depletion (wd), interannual variability (iv or iav), seasonal variable (sv or sev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25b7882d-b485-468c-9e59-ddc70c65eff9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d24cac-2ba5-4424-b0ee-5718c0ea59dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PATHS!\n",
    "# root\n",
    "rootPATH = r'\\Projections\\Final_Data\\Data'\n",
    "\n",
    "# 1. Hydrobasin 6 \n",
    "hy6PATH = os.path.join(rootPATH,  \"shapes\", \"hybas_lev06_v1c_merged_fiona_V04.shp\")\n",
    "\n",
    "# 2. Country boundaries\n",
    "ad0PATH = os.path.join(rootPATH,  \"shapes\", \"gadm36_0.shp\")\n",
    "\n",
    "# 3. Aqueduct 4.0 data\n",
    "aq4PATH = os.path.join(rootPATH, \"Aqueduct40\", \"step3_calculate_indicators\", \"final\", \"Aqueduct40_indicators_{}-additive.csv\").format\n",
    "\n",
    "# 4. Aqueduct 3.0 data\n",
    "aq3ROOT = r'C:\\Aqueduct3\\download\\Y2019M07D12_Aqueduct30_V01\\baseline'\n",
    "aq3PATH = r'C:Aqueduct3\\download\\Y2019M07D12_Aqueduct30_V01\\baseline\\annual\\arcmap\\y2019m07d12_aqueduct30_v01.gdb'\n",
    "aq3mPATH = r'C:\\Aqueduct3\\download\\Y2019M07D12_Aqueduct30_V01\\baseline\\monthly\\arcmap\\y2019m07d12_rh_aqueduct30_data_download_monthly_v01.shp'\n",
    "# 5. Category Weighting Scheme\n",
    "wghtPATH = os.path.join(rootPATH,  \"Aqueduct40\", \"util\", \"aq40_weights_enhanced.xlsx\")\n",
    "# 6. UTILITIY\n",
    "gdf_m_PATH = os.path.join(rootPATH,  \"Aqueduct40\", \"util\", 'aq40_monthly_geometry.shp')\n",
    "gdf_y_PATH = os.path.join(rootPATH,  \"Aqueduct40\", \"util\", 'aq40_annual_geometry.shp')\n",
    "\n",
    "# SAVE LOCATIONS\n",
    "outPATH = r'\\Projections\\Final_Data\\Data\\Aqueduct40\\step4_final_data_download'\n",
    "basePATH = os.path.join(outPATH, 'baseline', '{0}', 'y2023m07d05_sk_Aqueduct40_indicators_{0}.csv').format\n",
    "futPATH = os.path.join(outPATH, 'future', 'annual', 'y2023m07d05_sk_Aqueduct40_indicators_{}.csv').format\n",
    "cartoPATH = os.path.join(outPATH, 'carto', 'y2023m07d05_sk_Aqueduct40_indicators_{}.csv').format\n",
    "\n",
    "# GCM\n",
    "# PCR-GLOBWB Indicators\n",
    "gcmPATH = r'Projections\\Final_Data\\Data\\Aqueduct40\\step3_calculate_indicators'\n",
    "inPATH = os.path.join(gcmPATH, 'working', 'Aqueduct40_indicators_{}-additive.csv').format\n",
    "\n",
    "# Fix column names\n",
    "baseline_inds = {'ws': 'bws', 'wd': 'bwd', 'iv': 'iav', 'sv': 'sev'}\n",
    "baseline_typs = {'_r': '_raw', '_s': '_sco', '_c': '_cat', '_l': '_lab'}\n",
    "baseline_annual_typs = {'_r': '_raw', '_s': '_score', '_c': '_cat', '_l': '_label'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hy6 = gpd.read_file(hy6PATH)\n",
    "# PFAF FOUND TWICE: 353020 IN BOTH ASIA AND NORTHAMERICA (1 catchment cross 2 boundaries). Set all data = No Data.\n",
    "no_data_pfafs = [353020]\n",
    "hy6 = hy6.filter(['PFAF_ID', 'geometry']).set_index('PFAF_ID')\n",
    "hy6.index.name = 'pfaf_id'\n",
    "# Define projection from input data\n",
    "hy6_crs = hy6.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Baseline Monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm = pd.read_csv(aq4PATH('monthly'), index_col = 'pfaf_id')\n",
    "df_gbm = pd.merge(hy6, df_bm, how = 'left', left_index = True, right_index = True)\n",
    "# Set No Data as -9999\n",
    "df_gbm.replace(np.nan, -9999, inplace = True)\n",
    "# Relabel No Datas\n",
    "for l in [x for x in df_gbm.columns if \"lab\" in x]:\n",
    "    df_gbm[l].replace(-9999, 'No Data', inplace = True)\n",
    "\n",
    "# # Save CSV and Geodatabase version\n",
    "df_gbm = df_gbm.reset_index().set_index(['pfaf_id', 'year', 'month'])\n",
    "df_gbm.drop(['geometry'], axis = 1, inplace = True)\n",
    "df_gbm.to_csv(cartoPATH('monthly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            bws_raw                 bws_score               \\\n",
      "                                min          max          min          max   \n",
      "bws_label                                                                    \n",
      "Arid and Low Water Use     1.000000     1.000000     5.000000     5.000000   \n",
      "Extremely High (>80%)      0.800284  9999.000000     4.000512     5.000000   \n",
      "High (40-80%)              0.400018     0.799914     3.000065     3.999845   \n",
      "Low (<10%)                 0.000000     0.099996     0.000000     0.999937   \n",
      "Low - Medium (10-20%)      0.100002     0.199887     1.000028     1.999185   \n",
      "Medium - High (20-40%)     0.200032     0.399962     2.000231     2.999863   \n",
      "No Data                -9999.000000 -9999.000000 -9999.000000 -9999.000000   \n",
      "\n",
      "                       bws_cat          \n",
      "                           min     max  \n",
      "bws_label                               \n",
      "Arid and Low Water Use    -1.0    -1.0  \n",
      "Extremely High (>80%)      4.0     4.0  \n",
      "High (40-80%)              3.0     3.0  \n",
      "Low (<10%)                 0.0     0.0  \n",
      "Low - Medium (10-20%)      1.0     1.0  \n",
      "Medium - High (20-40%)     2.0     2.0  \n",
      "No Data                -9999.0 -9999.0  \n",
      "                            bwd_raw                 bwd_score               \\\n",
      "                                min          max          min          max   \n",
      "bwd_label                                                                    \n",
      "Arid and Low Water Use     1.000000     1.000000     5.000000     5.000000   \n",
      "Extremely High (>75%)      0.750095  9999.000000     4.000379     5.000000   \n",
      "High (50-75%)              0.500284     0.749752     3.001134     3.999010   \n",
      "Low (<5%)                  0.000000     0.049992     0.000000     0.999846   \n",
      "Low - Medium (5-25%)       0.050007     0.249757     1.000035     1.998785   \n",
      "Medium - High (25-50%)     0.250066     0.499093     2.000263     2.996373   \n",
      "No Data                -9999.000000 -9999.000000 -9999.000000 -9999.000000   \n",
      "\n",
      "                       bwd_cat          \n",
      "                           min     max  \n",
      "bwd_label                               \n",
      "Arid and Low Water Use    -1.0    -1.0  \n",
      "Extremely High (>75%)      4.0     4.0  \n",
      "High (50-75%)              3.0     3.0  \n",
      "Low (<5%)                  0.0     0.0  \n",
      "Low - Medium (5-25%)       1.0     1.0  \n",
      "Medium - High (25-50%)     2.0     2.0  \n",
      "No Data                -9999.0 -9999.0  \n",
      "                               iav_raw                 iav_score               \\\n",
      "                                   min          max          min          max   \n",
      "iav_label                                                                       \n",
      "Extremely High (>1.00)        1.000012     6.403124     4.000047     5.000000   \n",
      "High (0.75-1.00)              0.750002     0.999972     3.000009     3.999887   \n",
      "Low (<0.25)                   0.000000     0.249996     0.000000     0.999984   \n",
      "Low - Medium (0.25-0.50)      0.250001     0.499995     1.000004     1.999981   \n",
      "Medium - High (0.50-0.75)     0.500000     0.749994     2.000001     2.999975   \n",
      "No Data                   -9999.000000 -9999.000000 -9999.000000 -9999.000000   \n",
      "\n",
      "                          iav_cat          \n",
      "                              min     max  \n",
      "iav_label                                  \n",
      "Extremely High (>1.00)        4.0     4.0  \n",
      "High (0.75-1.00)              3.0     3.0  \n",
      "Low (<0.25)                   0.0     0.0  \n",
      "Low - Medium (0.25-0.50)      1.0     1.0  \n",
      "Medium - High (0.50-0.75)     2.0     2.0  \n",
      "No Data                   -9999.0 -9999.0  \n"
     ]
    }
   ],
   "source": [
    "# QA RESULTS\n",
    "inds = ['bws', 'bwd', 'iav']\n",
    "for i in inds:\n",
    "    l = \"{}_label\".format(i)\n",
    "    r = \"{}_raw\".format(i)\n",
    "    s = \"{}_score\".format(i)\n",
    "    d = \"{}_cat\".format(i)\n",
    "    df = df_gbm.groupby(l)[r, s, d].agg(['min', 'max'])\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rename columns using Aqueduct naming structure. \n",
    "ind_cols = df_gbm.columns.tolist()\n",
    "month_data = []\n",
    "# Loop by month\n",
    "for m in [str(x).zfill(2) for x in range(1,13)]:\n",
    "    # Filter dataframe to select 1 month at a time\n",
    "    df_f = df_gbm[df_gbm.index.get_level_values('month') == int(m)]\n",
    "    # Add month to column name (ex bws_raw becomes bws_01_raw)\n",
    "    mon_cols = [x[0:4] + m + x[3:] for x in ind_cols]\n",
    "    df_f.columns = mon_cols\n",
    "    # drop date details from index\n",
    "    df_clean = df_f.droplevel(level = ['month', 'year'])\n",
    "    # Append to list\n",
    "    month_data.append(df_clean)\n",
    "df_bm = pd.concat(month_data, axis = 1)\n",
    "df_bm.to_csv(basePATH('monthly'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Annual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in Aqueduct 3.0 database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_3 = gpd.read_file(aq3PATH, layer = 'annual')\n",
    "gdf_3.set_index(['string_id', 'aq30_id'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read in Aqueduct 4.0 annual results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = pd.read_csv(aq4PATH('annual'), index_col = 'pfaf_id')\n",
    "# Merge to spatial data (which will have more catchments than we had data for)\n",
    "df_gba = pd.merge(hy6, df_4, how = 'left', left_index = True, right_index = True)\n",
    "# Set No Data as -9999\n",
    "df_gba.replace(np.nan, -9999, inplace = True)\n",
    "# Relabel No Datas\n",
    "for l in [x for x in df_gba.columns if \"lab\" in x]:\n",
    "    df_gba[l].replace(-9999, 'No Data', inplace = True)\n",
    "    \n",
    "# Save CSV and Geodatabase version\n",
    "df_ba_final = df_gba.drop(['geometry'], axis = 1).sort_index()\n",
    "new_cols = df_ba_final.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Aq4 results to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_4 = gdf_3.copy()\n",
    "# Drop PCR-GLOBWB indicators\n",
    "gdf_4.drop(new_cols, axis = 1, inplace = True)\n",
    "# Drop w_awr columns to NaN\n",
    "gdf_4.drop([x for x in gdf_4.columns if \"w_awr\" in x], axis = 1, inplace = True)\n",
    "# Merge new data to dataframe based on catchment\n",
    "gdf_4 = pd.merge(gdf_4, df_ba_final, how = 'left', left_on = 'pfaf_id', right_index = True)\n",
    "# Replace missing data\n",
    "gdf_4.loc[:, [x for x in gdf_4.columns if \"_raw\" in x]] = gdf_4.loc[:, [x for x in gdf_4.columns if \"_raw\" in x]].fillna(-9999)\n",
    "gdf_4.loc[:, [x for x in gdf_4.columns if \"_score\" in x]] = gdf_4.loc[:, [x for x in gdf_4.columns if \"_score\" in x]].fillna(-9999)\n",
    "gdf_4.loc[:, [x for x in gdf_4.columns if \"_cat\" in x]] = gdf_4.loc[:, [x for x in gdf_4.columns if \"_cat\" in x]].fillna(-9999)\n",
    "gdf_4.loc[:, [x for x in gdf_4.columns if \"_label\" in x]] = gdf_4.loc[:, [x for x in gdf_4.columns if \"_label\" in x]].fillna('No Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fix Drought and GTD Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category(score, df_in):\n",
    "    cat = 'cat'\n",
    "    df_cat = np.floor(df_in[score]).to_frame(name = cat)\n",
    "    df_cat[cat] = np.where(df_cat[cat] == 5, 4, df_cat[cat])\n",
    "    return df_cat\n",
    "\n",
    "drr_labels = {\n",
    "    -9999 : 'No Data',\n",
    "    0: 'Low (0.0-0.2)',\n",
    "    1: 'Low - Medium (0.2-0.4)',\n",
    "    2: 'Medium (0.4-0.6)',\n",
    "    3: 'Medium - High (0.6-0.8)', \n",
    "    4: 'High (0.8-1.0)'\n",
    "}\n",
    "# Redo raw values. They were mistakenly set to equal score in Aq 3.0\n",
    "gdf_4['drr_raw'] = gdf_4['drr_score']/5\n",
    "# Limit to 0 to 5; set no data = -9999\n",
    "gdf_4['drr_raw'] = gdf_4['drr_raw'].mask(gdf_4['drr_score'] == -9999, -9999)\n",
    "gdf_4['drr_score'] = gdf_4['drr_score'].mask(gdf_4['drr_score'] < 0, 0)\n",
    "gdf_4['drr_score'] = gdf_4['drr_score'].mask(gdf_4['drr_score'] > 5, 5)\n",
    "gdf_4['drr_score'] = gdf_4['drr_score'].mask(gdf_4['drr_raw'] == -9999, -9999)\n",
    "# Set category\n",
    "gdf_4['drr_cat'] = category(score = 'drr_score', df_in = gdf_4)['cat']\n",
    "# Set Label\n",
    "gdf_4['drr_label'] = gdf_4['drr_cat'].map(drr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtd_score(r):\n",
    "    if np.isnan(r):\n",
    "        score = -9999\n",
    "    elif r == -9999:\n",
    "        score = -9999\n",
    "    elif r < 0:\n",
    "        score = max(0, r+1)\n",
    "    elif r < 2:\n",
    "        score = 0.5 * r + 1\n",
    "    elif r < 4:\n",
    "        score = 0.5 * r + 1\n",
    "    elif r < 8:\n",
    "        score = 0.25* r + 2\n",
    "    else:\n",
    "        score = min(0.25*r + 2, 5)\n",
    "    return score\n",
    "        \n",
    "gtd_labels = {\n",
    "    -9999 : 'Insignificant Trend',\n",
    "    0: 'Low (<0 cm/y)',\n",
    "    1: 'Low - Medium (0-2 cm/y)',\n",
    "    2: 'Medium - High (2-4 cm/y)',\n",
    "    3: 'High (4-8 cm/y)', \n",
    "    4: 'Extremely High (>8 cm/y)'\n",
    "}\n",
    "\n",
    "# Copy label. Will need to overwrite insignificant trends\n",
    "gdf_4['gtd_label_copy'] = gdf_4['gtd_label']\n",
    "gdf_4['gtd_score'] = gdf_4['gtd_raw'].apply(lambda x: gtd_score(x))\n",
    "# Set category\n",
    "gdf_4['gtd_cat'] = category(score = 'gtd_score', df_in = gdf_4)['cat']\n",
    "# Set Label\n",
    "gdf_4['gtd_label'] = gdf_4['gtd_cat'].map(gtd_labels)\n",
    "# Label insignificant trends\n",
    "gdf_4['gtd_label'] = gdf_4['gtd_label'].mask(gdf_4['gtd_label_copy'] == 'Insignificant Trend', 'Insignificant Trend')\n",
    "gdf_4['gtd_score'] = gdf_4['gtd_score'].mask(gdf_4['gtd_label_copy'] == 'Insignificant Trend', -9999)\n",
    "gdf_4['gtd_cat'] = gdf_4['gtd_cat'].mask(gdf_4['gtd_label_copy'] == 'Insignificant Trend', -9999)\n",
    "gdf_4.drop(['gtd_label_copy'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fix Coastal Flooding Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, set score = -1 if no risk\n",
    "gdf_4['cfr_score'] = gdf_4['cfr_score'].mask(gdf_4['cfr_raw'] == 0, 0)\n",
    "# Then, set category  = -1\n",
    "gdf_4['cfr_cat'] = gdf_4['cfr_cat'].mask(gdf_4['cfr_raw'] == 0, -1)\n",
    "# Then, create new label\n",
    "gdf_4['cfr_label'] = gdf_4['cfr_label'].mask(gdf_4['cfr_raw'] == 0, 'No Risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Add weighting scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scores(r, QS):\n",
    "    if np.isnan(r):\n",
    "        score = -9999\n",
    "    elif r < QS[1]:\n",
    "        score = (r - QS[0])/(QS[1] - QS[0]) + 0\n",
    "    elif r < QS[2]:\n",
    "        score = (r - QS[1])/(QS[2] - QS[1]) + 1\n",
    "    elif r < QS[3]:\n",
    "        score = (r - QS[2])/(QS[3] - QS[2]) + 2\n",
    "    elif r < QS[4]:\n",
    "        score = (r - QS[3])/(QS[4] - QS[3]) + 3       \n",
    "    elif r < QS[5]:\n",
    "        score = (r - QS[4])/(QS[5] - QS[4]) + 4\n",
    "    else:\n",
    "        score = 5\n",
    "    return score\n",
    "\n",
    "\n",
    "weight_labels = {\n",
    "    -9999 : 'No data',\n",
    "    0: 'Low (0-1)',\n",
    "    1: 'Low - Medium (1-2)',\n",
    "    2: 'Medium - High (2-3)',\n",
    "    3: 'High (3-4)', \n",
    "    4: 'Extremely High (4-5)'\n",
    "}\n",
    "\n",
    "# https://github.com/wri/aqueduct30_data_download/blob/master/metadata.md#quantile-linear-interpolation\n",
    "qan = [0, 0.72, 1.09, 1.60, 2.34, 5]\n",
    "qal = [0, 1.45, 2.20, 2.92, 3.83, 5]\n",
    "rrr = [0, 0.30, 1.39, 2.81, 3.93, 5]\n",
    "ovr = [0, 1.01, 1.61, 2.10, 2.68, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods reflect the approach taken in Aqueduct 3.0\n",
    "There are three things we need to calculate:\n",
    "\n",
    "1. Weighted group averages per industry. Find the fraction that each indicator represents within the group (QAN, QAL, RRR) (first, indicators with no data are dropped)\n",
    "\n",
    "2. Weighted overall averages per industry (looking at all indicators).  First, we need to find the fraction that each indicator represents within the overall (all 13 indicators) (first, indicators with no data are dropped)\n",
    "\n",
    "3. Fraction of indicators present. The averages have dropped null values. If too many indicators are missing, we need to mask out the overall scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of locations without enough data to average: 16.75459083875865\n",
      "def\n",
      "agr\n",
      "fnb\n",
      "che\n",
      "elp\n",
      "smc\n",
      "ong\n",
      "min\n",
      "con\n",
      "tex\n"
     ]
    }
   ],
   "source": [
    "# First, read in weights\n",
    "df_w = pd.read_excel(wghtPATH, sheet_name = 'export_csv')\n",
    "df_w = df_w.filter(['group_short', 'industry_short', 'indicator_short', 'weight_abs', 'weight_fraction'])\n",
    "\n",
    "# Select only the score values from the full dataset and turn indicators from columns to rows\n",
    "df_raw = gdf_4.loc[:,[x for x in gdf_4.columns if \"_score\" in x]]\n",
    "df_raw = df_raw.melt(ignore_index = False).reset_index()\n",
    "# Create new upper case version of indicator name (ex: bws_score = BWS)\n",
    "df_raw['indicator'] = df_raw['variable'].apply(lambda x: x[0:3].upper())\n",
    "# Replace no datas with -9999 and 9999 to 5\n",
    "df_raw.replace(-9999, np.nan, inplace = True)\n",
    "df_raw['value'] = df_raw['value'].mask(df_raw['value'] > 5, 5)\n",
    "# Merge weights to each indicator for every geometry\n",
    "df_merge = pd.merge(df_w, df_raw, how = 'outer', left_on = 'indicator_short', right_on = 'indicator')\n",
    "\n",
    "# 1. & 2. WEIGHTED GROUP AVERAGES PER INDUSTRY & OVERALL AVERAGES PER INDUSTRY\n",
    "# Set weights equal to NAN if no data is available\n",
    "df_merge.loc[df_merge['value'].isna(), ['weight_abs', 'weight_fraction']] = np.nan\n",
    "# Find weight fraction per group and per overall\n",
    "# Find total weight per group/cat; and per cat (aka overall)\n",
    "group_weights = df_merge.groupby(['string_id', 'aq30_id', 'group_short', 'industry_short'])['weight_abs'].sum().to_frame(name = 'grp_wght')\n",
    "overall_weights = df_merge.groupby(['string_id', 'aq30_id', 'industry_short'])['weight_abs'].sum().to_frame(name = 'tot_wght')\n",
    "# Merge to data to find fractions\n",
    "df_merge = pd.merge(df_merge, group_weights, how = 'left', left_on = ['string_id', 'aq30_id', 'group_short', 'industry_short'], right_index = True)\n",
    "df_merge = pd.merge(df_merge, overall_weights, how = 'left', left_on = ['string_id', 'aq30_id', 'industry_short'], right_index = True)\n",
    "# Calculate fraction for group and overall\n",
    "df_merge['grp_fraction'] = df_merge['weight_abs'].divide(df_merge['grp_wght'])\n",
    "df_merge['tot_fraction'] = df_merge['weight_abs'].divide(df_merge['tot_wght'])\n",
    "# Multiply the value by the weight\n",
    "df_merge['grp_value'] = df_merge['grp_fraction'].multiply(df_merge['value'])\n",
    "df_merge['tot_value'] = df_merge['tot_fraction'].multiply(df_merge['value'])\n",
    "# Sum weighted values by group-industry; standarized name of fraction and value columns\n",
    "df_avg_grp = df_merge.groupby(['string_id', 'aq30_id', 'group_short', 'industry_short'])[['grp_fraction', 'grp_value']].sum().reset_index()\n",
    "df_avg_grp.rename(columns = {'grp_fraction': 'fraction', 'grp_value': 'value'}, inplace = True)\n",
    "# Sum weighted values by overall industry; standarized name of fraction and value columns\n",
    "df_avg_tot = df_merge.groupby(['string_id', 'aq30_id', 'industry_short'])[['tot_fraction', 'tot_value']].sum().reset_index()\n",
    "df_avg_tot.rename(columns = {'tot_fraction': 'fraction', 'tot_value': 'value'}, inplace = True)\n",
    "df_avg_tot['group_short'] = 'TOT'\n",
    "# Pivot the data so Groups are columns and industries are rows (with string ID)\n",
    "df_piv_grp = pd.pivot_table(data = df_avg_grp, values = ['fraction', 'value'], index = ['string_id', 'aq30_id', 'industry_short'], columns = ['group_short'], aggfunc ='sum')\n",
    "df_piv_tot = pd.pivot_table(data = df_avg_tot, values = ['fraction', 'value'], index = ['string_id', 'aq30_id', 'industry_short'], columns = ['group_short'], aggfunc ='sum')\n",
    "# Merge together\n",
    "df_piv = pd.concat([df_piv_grp, df_piv_tot], axis = 1)\n",
    "# Just keep values\n",
    "df_val = df_piv['value']\n",
    "# Rename to raw\n",
    "df_val = df_val.add_suffix('_RAW')\n",
    "# Calculate score\n",
    "df_val['QAL_SCORE'] = df_val['QAL_RAW'].apply(lambda x: weight_scores(x, qal))\n",
    "df_val['QAN_SCORE'] = df_val['QAN_RAW'].apply(lambda x: weight_scores(x, qan))\n",
    "df_val['RRR_SCORE'] = df_val['RRR_RAW'].apply(lambda x: weight_scores(x, rrr))\n",
    "df_val['TOT_SCORE'] = df_val['TOT_RAW'].apply(lambda x: weight_scores(x, ovr))\n",
    "\n",
    "# Create categories\n",
    "for i in ['QAL', 'QAN', 'RRR', 'TOT']:\n",
    "    sc = i + \"_SCORE\"\n",
    "    ct = i + \"_CAT\"\n",
    "    lb = i + \"_LABEL\"\n",
    "    df_val[ct] = category(score = sc, df_in = df_val)['cat']\n",
    "    df_val[lb] = df_val[ct].map(weight_labels)\n",
    "\n",
    "    \n",
    "# 3. FRACTION OF DATA AVAILABILE\n",
    "frac_grp = df_merge.groupby(['string_id', 'aq30_id', 'group_short', 'industry_short'])['weight_fraction'].sum().to_frame(name = 'value').reset_index()\n",
    "frac_grp['group_short'] = frac_grp['group_short'].apply(lambda x: x + \"WEIGHT_FRACTION\")\n",
    "frac_tot = df_merge.groupby(['string_id', 'aq30_id',  'industry_short'])['weight_fraction'].sum().to_frame(name = 'value').reset_index()\n",
    "frac_tot['group_short'] = 'TOT_WEIGHT_FRACTION'\n",
    "print(\"Percentage of locations without enough data to average:\", len(frac_tot[frac_tot.value < 0.75]) / len(frac_tot)*100)\n",
    "# Melt the values data\n",
    "df_val_melt = df_val.melt(ignore_index = False).reset_index()\n",
    "# Combine values and fractions\n",
    "df_combo = pd.concat([df_val_melt, frac_grp, frac_tot], axis = 0)\n",
    "df_combo['column'] = 'W_AWR_' + df_combo['industry_short'] + \"_\" + df_combo['group_short'] \n",
    "\n",
    "# Flip data so group-industry-indicators are columns\n",
    "df_warw = pd.pivot(data = df_combo, values = 'value', columns = 'column', index = ['string_id', 'aq30_id'])\n",
    "df_warw.columns = [x.lower() for x in df_warw.columns]\n",
    "\n",
    "# Set values to floats\n",
    "for i in df_warw.columns:\n",
    "    if \"label\" not in i:\n",
    "        df_warw[i] = df_warw[i].astype(float)\n",
    "        \n",
    "# If overall fraction for group is less than 0.75, set to no data\n",
    "industry_list = [x.lower() for x in list(df_w['industry_short'].unique())]\n",
    "for i in industry_list:\n",
    "    print(i)\n",
    "    # Create fraction name\n",
    "    fr = 'w_awr_{}_tot_weight_fraction'.format(i)\n",
    "    # Find all industry-related scores, raws, and cats\n",
    "    num_cols = ['w_awr_{}_{}_{}'.format(i, t, c) for c in ['raw', 'score', 'cat'] for t in ['qan', 'qal', 'rrr', 'tot']]\n",
    "    # Find all industry-related labels\n",
    "    lab_cols = ['w_awr_{}_{}_label'.format(i, t) for t in ['qan', 'qal', 'rrr', 'tot']]\n",
    "    # Set numbers = -9999 and label to No Data\n",
    "    df_warw.loc[df_warw[fr] <0.75, num_cols] = -9999\n",
    "    df_warw.loc[df_warw[fr] <0.75, lab_cols] = 'No data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create CARTO table for weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_groups = {'bws': 'qan', 'bwd': 'qan', 'cfr': 'qan', 'rfr': 'qan', \n",
    "              'iav': 'qan', 'sev': 'qan',  'gtd': 'qan', 'drr': 'qan',\n",
    "              'cep': 'qal', 'ucw': 'qal', \n",
    "              'rri': 'rrr', 'udw': 'rrr', 'usa': 'rrr'}\n",
    "\n",
    "# Only keep indicator scores\n",
    "df_c = gdf_4.loc[:, [x for x in gdf_4.columns if \"score\" in x]]\n",
    "# Pivot table so columns become rows\n",
    "df_cm = df_c.melt(ignore_index = False)\n",
    "# Drop no datas\n",
    "df_cm = df_cm.loc[df_cm['value']!= -9999, :]\n",
    "# Turn old column name into just indicator abbr\n",
    "df_cm['indicator'] = df_cm['variable'].apply(lambda x: x[0:3])\n",
    "df_cm['group_short'] = df_cm['indicator'].apply(lambda x: cat_groups.get(x))\n",
    "df_cm.rename(columns = {'value': 'score'}, inplace = True)\n",
    "df_cm.drop('variable', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Merge all pieces together, and sort to match original Aq 3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4_final = pd.concat([gdf_4, df_warw], axis = 1)\n",
    "df_4_final = df_4_final.filter(gdf_3.columns[0:-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV and Geodatabase version\n",
    "df_4_final.to_csv(basePATH('annual'))\n",
    "df_4_final.to_csv(cartoPATH('annual'))\n",
    "df_cm.to_csv(cartoPATH('custom'))\n",
    "# DROP GEOM AND SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save location ID and geometry to separate separate shapefile to rebuild as geodatabase in ArcMAP\n",
    "# gdf_shp = gdf_3.filter(['Shape_Length', 'Shape_Area', 'geometry'])\n",
    "# # Define projection from input data\n",
    "# aq_crs = gdf_3.crs \n",
    "# gdf_shp.to_file(gdf_y_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BWS', 'BWD', 'GTD', 'IAV', 'SEV', 'DRR', 'RFR', 'CFR', 'UCW',\n",
       "       'CEP', 'UDW', 'USA', 'RRI'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w.indicator_short.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            bws_raw                 bws_score               \\\n",
      "                                min          max          min          max   \n",
      "bws_label                                                                    \n",
      "Arid and Low Water Use     1.000000     1.000000     5.000000     5.000000   \n",
      "Extremely High (>80%)      0.803479  9999.000000     4.006260     5.000000   \n",
      "High (40-80%)              0.400318     0.799223     3.001146     3.998599   \n",
      "Low (<10%)                 0.000000     0.099904     0.000000     0.998614   \n",
      "Low - Medium (10-20%)      0.100020     0.199952     1.000294     1.999654   \n",
      "Medium - High (20-40%)     0.200010     0.399624     2.000069     2.998643   \n",
      "No Data                -9999.000000 -9999.000000 -9999.000000 -9999.000000   \n",
      "\n",
      "                       bws_cat          \n",
      "                           min     max  \n",
      "bws_label                               \n",
      "Arid and Low Water Use    -1.0    -1.0  \n",
      "Extremely High (>80%)      4.0     4.0  \n",
      "High (40-80%)              3.0     3.0  \n",
      "Low (<10%)                 0.0     0.0  \n",
      "Low - Medium (10-20%)      1.0     1.0  \n",
      "Medium - High (20-40%)     2.0     2.0  \n",
      "No Data                -9999.0 -9999.0  \n",
      "                            bwd_raw                 bwd_score               \\\n",
      "                                min          max          min          max   \n",
      "bwd_label                                                                    \n",
      "Arid and Low Water Use     1.000000     1.000000     5.000000     5.000000   \n",
      "Extremely High (>75%)      0.753574  9999.000000     4.014296     5.000000   \n",
      "High (50-75%)              0.501425     0.748618     3.005702     3.994472   \n",
      "Low (<5%)                  0.000000     0.049957     0.000000     0.999132   \n",
      "Low - Medium (5-25%)       0.050002     0.249414     1.000012     1.997069   \n",
      "Medium - High (25-50%)     0.250685     0.499859     2.002740     2.999435   \n",
      "No Data                -9999.000000 -9999.000000 -9999.000000 -9999.000000   \n",
      "\n",
      "                       bwd_cat          \n",
      "                           min     max  \n",
      "bwd_label                               \n",
      "Arid and Low Water Use    -1.0    -1.0  \n",
      "Extremely High (>75%)      4.0     4.0  \n",
      "High (50-75%)              3.0     3.0  \n",
      "Low (<5%)                  0.0     0.0  \n",
      "Low - Medium (5-25%)       1.0     1.0  \n",
      "Medium - High (25-50%)     2.0     2.0  \n",
      "No Data                -9999.0 -9999.0  \n",
      "                               gtd_raw               gtd_score               \\\n",
      "                                   min        max          min          max   \n",
      "gtd_label                                                                     \n",
      "Extremely High (>8 cm/y)  8.842760e+00  44.097800     4.210690     5.000000   \n",
      "High (4-8 cm/y)           4.597100e+00   6.781250     3.149275     3.695312   \n",
      "Insignificant Trend      -9.999000e+03  18.517600 -9999.000000 -9999.000000   \n",
      "Low (<0 cm/y)            -1.014470e+01  -0.007024     0.000000     0.992976   \n",
      "Low - Medium (0-2 cm/y)   8.265460e-12   1.891740     1.000000     1.945870   \n",
      "Medium - High (2-4 cm/y)  2.054730e+00   3.855870     2.027365     2.927935   \n",
      "\n",
      "                         gtd_cat          \n",
      "                             min     max  \n",
      "gtd_label                                 \n",
      "Extremely High (>8 cm/y)     4.0     4.0  \n",
      "High (4-8 cm/y)              3.0     3.0  \n",
      "Insignificant Trend      -9999.0 -9999.0  \n",
      "Low (<0 cm/y)                0.0     0.0  \n",
      "Low - Medium (0-2 cm/y)      1.0     1.0  \n",
      "Medium - High (2-4 cm/y)     2.0     2.0  \n",
      "                               iav_raw                 iav_score               \\\n",
      "                                   min          max          min          max   \n",
      "iav_label                                                                       \n",
      "Extremely High (>1.00)        1.000015     6.403124     4.000061     5.000000   \n",
      "High (0.75-1.00)              0.750214     0.999526     3.000857     3.998106   \n",
      "Low (<0.25)                   0.001190     0.249901     0.004761     0.999603   \n",
      "Low - Medium (0.25-0.50)      0.250089     0.499986     1.000357     1.999943   \n",
      "Medium - High (0.50-0.75)     0.500051     0.749909     2.000205     2.999637   \n",
      "No Data                   -9999.000000 -9999.000000 -9999.000000 -9999.000000   \n",
      "\n",
      "                          iav_cat          \n",
      "                              min     max  \n",
      "iav_label                                  \n",
      "Extremely High (>1.00)        4.0     4.0  \n",
      "High (0.75-1.00)              3.0     3.0  \n",
      "Low (<0.25)                   0.0     0.0  \n",
      "Low - Medium (0.25-0.50)      1.0     1.0  \n",
      "Medium - High (0.50-0.75)     2.0     2.0  \n",
      "No Data                   -9999.0 -9999.0  \n",
      "                               sev_raw                 sev_score               \\\n",
      "                                   min          max          min          max   \n",
      "sev_label                                                                       \n",
      "Extremely High (>1.33)        1.333930     3.464102     4.001789     5.000000   \n",
      "High (1.00-1.33)              1.000490     1.333150     3.001470     3.999449   \n",
      "Low (<0.33)                   0.013436     0.333296     0.040309     0.999888   \n",
      "Low - Medium (0.33-0.66)      0.333392     0.666339     1.000177     1.999018   \n",
      "Medium - High (0.66-1.00)     0.666947     0.999929     2.000840     2.999786   \n",
      "No Data                   -9999.000000 -9999.000000 -9999.000000 -9999.000000   \n",
      "\n",
      "                          sev_cat          \n",
      "                              min     max  \n",
      "sev_label                                  \n",
      "Extremely High (>1.33)        4.0     4.0  \n",
      "High (1.00-1.33)              3.0     3.0  \n",
      "Low (<0.33)                   0.0     0.0  \n",
      "Low - Medium (0.33-0.66)      1.0     1.0  \n",
      "Medium - High (0.66-1.00)     2.0     2.0  \n",
      "No Data                   -9999.0 -9999.0  \n",
      "                             drr_raw                 drr_score               \\\n",
      "                                 min          max          min          max   \n",
      "drr_label                                                                     \n",
      "High (0.8-1.0)              0.801039     4.800975     4.005197     5.000000   \n",
      "Low (0.0-0.2)               0.004345     0.199796     0.021723     0.998980   \n",
      "Low - Medium (0.2-0.4)      0.200234     0.399564     1.001169     1.997822   \n",
      "Medium (0.4-0.6)            0.400566     0.599459     2.002831     2.997294   \n",
      "Medium - High (0.6-0.8)     0.600078     0.799437     3.000391     3.997183   \n",
      "No Data                 -9999.000000 -9999.000000 -9999.000000 -9999.000000   \n",
      "\n",
      "                        drr_cat          \n",
      "                            min     max  \n",
      "drr_label                                \n",
      "High (0.8-1.0)              4.0     4.0  \n",
      "Low (0.0-0.2)               0.0     0.0  \n",
      "Low - Medium (0.2-0.4)      1.0     1.0  \n",
      "Medium (0.4-0.6)            2.0     2.0  \n",
      "Medium - High (0.6-0.8)     3.0     3.0  \n",
      "No Data                 -9999.0 -9999.0  \n",
      "                                              rfr_raw               \\\n",
      "                                                  min          max   \n",
      "rfr_label                                                            \n",
      "Extremely High (more than 1 in 100)          0.013612     0.349990   \n",
      "High (6 in 1,000 to 1 in 100)                0.006162     0.013606   \n",
      "Low (0 to 1 in 1,000)                        0.000000     0.001181   \n",
      "Low - Medium (1 in 1,000 to 2 in 1,000)      0.001182     0.002987   \n",
      "Medium - High (2 in 1,000 to 6 in 1,000)     0.002987     0.006161   \n",
      "No Data                                  -9999.000000 -9999.000000   \n",
      "\n",
      "                                            rfr_score              rfr_cat  \\\n",
      "                                                  min          max     min   \n",
      "rfr_label                                                                    \n",
      "Extremely High (more than 1 in 100)          4.000016     5.000000     4.0   \n",
      "High (6 in 1,000 to 1 in 100)                3.000091     3.999825     3.0   \n",
      "Low (0 to 1 in 1,000)                        0.000000     0.999539     0.0   \n",
      "Low - Medium (1 in 1,000 to 2 in 1,000)      1.000075     1.999960     1.0   \n",
      "Medium - High (2 in 1,000 to 6 in 1,000)     2.000015     2.999858     2.0   \n",
      "No Data                                  -9999.000000 -9999.000000 -9999.0   \n",
      "\n",
      "                                                  \n",
      "                                             max  \n",
      "rfr_label                                         \n",
      "Extremely High (more than 1 in 100)          4.0  \n",
      "High (6 in 1,000 to 1 in 100)                3.0  \n",
      "Low (0 to 1 in 1,000)                        0.0  \n",
      "Low - Medium (1 in 1,000 to 2 in 1,000)      1.0  \n",
      "Medium - High (2 in 1,000 to 6 in 1,000)     2.0  \n",
      "No Data                                  -9999.0  \n",
      "                                                    cfr_raw               \\\n",
      "                                                        min          max   \n",
      "cfr_label                                                                  \n",
      "Extremely High (more than 2 in 1,000)          2.158230e-03     0.359994   \n",
      "High (3 in 10,000 to 2 in 1,000)               3.601991e-04     0.002148   \n",
      "Low (0 to 9 in 1,000,000)                      1.020827e-09     0.000010   \n",
      "Low - Medium (9 in 1,000,000 to 7 in 100,000)  9.689931e-06     0.000071   \n",
      "Medium - High (7 in 100,000 to 3 in 10,000)    7.152439e-05     0.000356   \n",
      "No Data                                       -9.999000e+03 -9999.000000   \n",
      "No Risk                                        0.000000e+00     0.000000   \n",
      "\n",
      "                                                 cfr_score               \\\n",
      "                                                       min          max   \n",
      "cfr_label                                                                 \n",
      "Extremely High (more than 2 in 1,000)             4.000012     5.000000   \n",
      "High (3 in 10,000 to 2 in 1,000)                  3.001837     3.996524   \n",
      "Low (0 to 9 in 1,000,000)                         0.000106     0.997445   \n",
      "Low - Medium (9 in 1,000,000 to 7 in 100,000)     1.000598     1.998165   \n",
      "Medium - High (7 in 100,000 to 3 in 10,000)       2.000099     2.997108   \n",
      "No Data                                       -9999.000000 -9999.000000   \n",
      "No Risk                                           0.000000     0.000000   \n",
      "\n",
      "                                              cfr_cat          \n",
      "                                                  min     max  \n",
      "cfr_label                                                      \n",
      "Extremely High (more than 2 in 1,000)             4.0     4.0  \n",
      "High (3 in 10,000 to 2 in 1,000)                  3.0     3.0  \n",
      "Low (0 to 9 in 1,000,000)                         0.0     0.0  \n",
      "Low - Medium (9 in 1,000,000 to 7 in 100,000)     1.0     1.0  \n",
      "Medium - High (7 in 100,000 to 3 in 10,000)       2.0     2.0  \n",
      "No Data                                       -9999.0 -9999.0  \n",
      "No Risk                                          -1.0    -1.0  \n",
      "                                    ucw_raw                 ucw_score  \\\n",
      "                                        min          max          min   \n",
      "ucw_label                                                               \n",
      "Extremely High (100%)              0.999834     1.000000     4.000000   \n",
      "High (90-100%)                     0.901912     0.999412     3.019152   \n",
      "Low (<30%)                         0.000000     0.294616     0.000000   \n",
      "Low - Medium (30-60%)              0.300000     0.590000     1.000000   \n",
      "Medium - High (60-90%)             0.600000     0.876272     2.000000   \n",
      "No Data                        -9999.000000 -9999.000000 -9999.000000   \n",
      "No to Low Wastewater Collected    -1.000000    -1.000000     5.000000   \n",
      "\n",
      "                                            ucw_cat          \n",
      "                                        max     min     max  \n",
      "ucw_label                                                    \n",
      "Extremely High (100%)              5.000000     4.0     4.0  \n",
      "High (90-100%)                     3.995773     3.0     3.0  \n",
      "Low (<30%)                         0.982053     0.0     0.0  \n",
      "Low - Medium (30-60%)              1.966667     1.0     1.0  \n",
      "Medium - High (60-90%)             2.920907     2.0     2.0  \n",
      "No Data                        -9999.000000 -9999.0 -9999.0  \n",
      "No to Low Wastewater Collected     5.000000    -1.0    -1.0  \n",
      "                            cep_raw                 cep_score               \\\n",
      "                                min          max          min          max   \n",
      "cep_label                                                                    \n",
      "Extremely High (>5)        5.010954    80.416366     4.000145     5.000000   \n",
      "High (1 to 5)              1.000007     4.989168     3.000002     3.997292   \n",
      "Low (<-5)               -240.620164    -5.012115     0.000000     0.999949   \n",
      "Low - Medium (-5 to 0)    -4.990653    -0.000013     1.001869     1.999997   \n",
      "Medium - High (0 to 1)     0.000000     0.999293     2.000000     2.999293   \n",
      "No Data                -9999.000000 -9999.000000 -9999.000000 -9999.000000   \n",
      "\n",
      "                       cep_cat          \n",
      "                           min     max  \n",
      "cep_label                               \n",
      "Extremely High (>5)        4.0     4.0  \n",
      "High (1 to 5)              3.0     3.0  \n",
      "Low (<-5)                  0.0     0.0  \n",
      "Low - Medium (-5 to 0)     1.0     1.0  \n",
      "Medium - High (0 to 1)     2.0     2.0  \n",
      "No Data                -9999.0 -9999.0  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           udw_raw                 udw_score               \\\n",
      "                               min          max          min          max   \n",
      "udw_label                                                                   \n",
      "Extremely High (>20%)     0.200096     0.693025     4.000696     5.000000   \n",
      "High (10-20%)             0.100019     0.199712     3.000274     3.997923   \n",
      "Low (<2.5%)               0.000000     0.024995     0.000000     0.999687   \n",
      "Low - Medium (2.5-5%)     0.025016     0.049902     1.000898     1.997177   \n",
      "Medium - High (5-10%)     0.050041     0.099957     2.001173     2.999385   \n",
      "No Data               -9999.000000 -9999.000000 -9999.000000 -9999.000000   \n",
      "\n",
      "                      udw_cat          \n",
      "                          min     max  \n",
      "udw_label                              \n",
      "Extremely High (>20%)     4.0     4.0  \n",
      "High (10-20%)             3.0     3.0  \n",
      "Low (<2.5%)               0.0     0.0  \n",
      "Low - Medium (2.5-5%)     1.0     1.0  \n",
      "Medium - High (5-10%)     2.0     2.0  \n",
      "No Data               -9999.0 -9999.0  \n",
      "                           usa_raw                 usa_score               \\\n",
      "                               min          max          min          max   \n",
      "usa_label                                                                   \n",
      "Extremely High (>20%)     0.200339     0.962878     4.002440     5.000000   \n",
      "High (10-20%)             0.100129     0.199774     3.001853     3.998367   \n",
      "Low (<2.5%)               0.000000     0.024985     0.000000     0.999151   \n",
      "Low - Medium (2.5-5%)     0.025045     0.049726     1.002604     1.992082   \n",
      "Medium - High (5-10%)     0.050267     0.099999     2.007694     2.999989   \n",
      "No Data               -9999.000000 -9999.000000 -9999.000000 -9999.000000   \n",
      "\n",
      "                      usa_cat          \n",
      "                          min     max  \n",
      "usa_label                              \n",
      "Extremely High (>20%)     4.0     4.0  \n",
      "High (10-20%)             3.0     3.0  \n",
      "Low (<2.5%)               0.0     0.0  \n",
      "Low - Medium (2.5-5%)     1.0     1.0  \n",
      "Medium - High (5-10%)     2.0     2.0  \n",
      "No Data               -9999.0 -9999.0  \n",
      "                       rri_raw         rri_score              rri_cat        \n",
      "                           min     max       min          max     min     max\n",
      "rri_label                                                                    \n",
      "Extremely High (>75%)     75.0   100.0       4.0     5.000000     4.0     4.0\n",
      "High (60-75%)             60.0    74.0       3.0     3.933333     3.0     3.0\n",
      "Low (<25%)                 0.0    24.0       0.0     0.960000     0.0     0.0\n",
      "Low - Medium (25-50%)     25.0    49.0       1.0     1.960000     1.0     1.0\n",
      "Medium - High (50-60%)    50.0    59.0       2.0     2.900000     2.0     2.0\n",
      "No Data                -9999.0 -9999.0   -9999.0 -9999.000000 -9999.0 -9999.0\n"
     ]
    }
   ],
   "source": [
    "inds = ['bws', 'bwd', 'gtd', 'iav', 'sev', 'drr', 'rfr', 'cfr', 'ucw',\n",
    "       'cep', 'udw', 'usa', 'rri']\n",
    "for i in inds:\n",
    "    df = df_4_final.groupby(i + \"_label\")[i + '_raw', i + \"_score\", i + '_cat'].agg(['min', 'max'])\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA DOWLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in monthly data\n",
    "df_4f = pd.read_csv(aq4PATH('future'), index_col = 'pfaf_id')\n",
    "# Read in Aqueduct 3 monthly\n",
    "gdf_3m = gpd.read_file(aq3mPATH)\n",
    "gdf_3m.set_index(['fid', 'pfaf_id'], inplace = True)\n",
    "# Set to same shape as Aq 3 data\n",
    "df_4f_final = pd.DataFrame(index = gdf_3m.index).reset_index()\n",
    "df_4f_final = pd.merge(df_4f_final, df_4f, how = 'left', left_on = 'pfaf_id', right_index = True).set_index(['fid', 'pfaf_id'])\n",
    "df_4f_final.to_csv(futPATH('future'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       bau30_ws_x_r              bau30_ws_x_s            \\\n",
      "                                min          max          min       max   \n",
      "bau30_ws_x_l                                                              \n",
      "Arid and low water use     1.000000     1.000000     5.000000  5.000000   \n",
      "Extremely high (>80%)      0.802074  9999.000000     4.003736  5.000000   \n",
      "High (40-80%)              0.400100     0.799942     3.000360  3.999896   \n",
      "Low (<10%)                 0.000000     0.099997     0.000000  0.999958   \n",
      "Low-medium (10-20%)        0.100267     0.199945     1.003844  1.999603   \n",
      "Medium-high (20-40%)       0.200328     0.399897     2.002361  2.999628   \n",
      "\n",
      "                       bau30_ws_x_c       \n",
      "                                min  max  \n",
      "bau30_ws_x_l                              \n",
      "Arid and low water use         -1.0 -1.0  \n",
      "Extremely high (>80%)           4.0  4.0  \n",
      "High (40-80%)                   3.0  3.0  \n",
      "Low (<10%)                      0.0  0.0  \n",
      "Low-medium (10-20%)             1.0  1.0  \n",
      "Medium-high (20-40%)            2.0  2.0  \n",
      "                       bau30_wd_x_r              bau30_wd_x_s            \\\n",
      "                                min          max          min       max   \n",
      "bau30_wd_x_l                                                              \n",
      "Arid and Low Water Use     1.000000     1.000000     5.000000  5.000000   \n",
      "Extremely High (>75%)      0.750275  9999.000000     4.001099  5.000000   \n",
      "High (50-75%)              0.500022     0.747267     3.000089  3.989070   \n",
      "Low (<5%)                  0.000000     0.049964     0.000000  0.999286   \n",
      "Low - Medium (5-25%)       0.050068     0.249988     1.000340  1.999942   \n",
      "Medium - High (25-50%)     0.250329     0.498959     2.001316  2.995834   \n",
      "\n",
      "                       bau30_wd_x_c       \n",
      "                                min  max  \n",
      "bau30_wd_x_l                              \n",
      "Arid and Low Water Use         -1.0 -1.0  \n",
      "Extremely High (>75%)           4.0  4.0  \n",
      "High (50-75%)                   3.0  3.0  \n",
      "Low (<5%)                       0.0  0.0  \n",
      "Low - Medium (5-25%)            1.0  1.0  \n",
      "Medium - High (25-50%)          2.0  2.0  \n",
      "                          bau30_iv_x_r              bau30_iv_x_s               \\\n",
      "                                   min          max          min          max   \n",
      "bau30_iv_x_l                                                                    \n",
      "Extremely High (>1.00)        1.000097     5.567764     4.000389     5.000000   \n",
      "High (0.75-1.00)              0.750114     0.999845     3.000455     3.999378   \n",
      "Low (<0.25)                   0.001218     0.249965     0.004871     0.999861   \n",
      "Low - Medium (0.25-0.50)      0.250058     0.499957     1.000231     1.999828   \n",
      "Medium - High (0.50-0.75)     0.500053     0.749911     2.000211     2.999645   \n",
      "No Data                   -9999.000000 -9999.000000 -9999.000000 -9999.000000   \n",
      "\n",
      "                          bau30_iv_x_c          \n",
      "                                   min     max  \n",
      "bau30_iv_x_l                                    \n",
      "Extremely High (>1.00)             4.0     4.0  \n",
      "High (0.75-1.00)                   3.0     3.0  \n",
      "Low (<0.25)                        0.0     0.0  \n",
      "Low - Medium (0.25-0.50)           1.0     1.0  \n",
      "Medium - High (0.50-0.75)          2.0     2.0  \n",
      "No Data                        -9999.0 -9999.0  \n",
      "                          bau30_sv_x_r              bau30_sv_x_s               \\\n",
      "                                   min          max          min          max   \n",
      "bau30_sv_x_l                                                                    \n",
      "Extremely High (>1.33)        1.333473     3.431300     4.000420     5.000000   \n",
      "High (1.00-1.33)              1.000073     1.331823     3.000219     3.995468   \n",
      "Low (<0.33)                   0.011304     0.333316     0.033911     0.999948   \n",
      "Low - Medium (0.33-0.66)      0.333352     0.666647     1.000056     1.999941   \n",
      "Medium - High (0.66-1.00)     0.666719     0.999872     2.000158     2.999617   \n",
      "No Data                   -9999.000000 -9999.000000 -9999.000000 -9999.000000   \n",
      "\n",
      "                          bau30_sv_x_c          \n",
      "                                   min     max  \n",
      "bau30_sv_x_l                                    \n",
      "Extremely High (>1.33)             4.0     4.0  \n",
      "High (1.00-1.33)                   3.0     3.0  \n",
      "Low (<0.33)                        0.0     0.0  \n",
      "Low - Medium (0.33-0.66)           1.0     1.0  \n",
      "Medium - High (0.66-1.00)          2.0     2.0  \n",
      "No Data                        -9999.0 -9999.0  \n"
     ]
    }
   ],
   "source": [
    "inds = ['ws', 'wd', 'iv', 'sv']\n",
    "years = ['30']\n",
    "scens = ['bau']\n",
    "\n",
    "for i in inds:\n",
    "    for s in scens:\n",
    "        for y in years:\n",
    "            la = '{}{}_{}_x_l'.format(s,y,i)\n",
    "            ra = '{}{}_{}_x_r'.format(s,y,i)\n",
    "            sc = '{}{}_{}_x_s'.format(s,y,i)\n",
    "            ca = '{}{}_{}_x_c'.format(s,y,i)\n",
    "            df = df_4f_final.groupby(la)[ra, sc, ca].agg(['min', 'max'])\n",
    "            print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             min     max\n",
      "bau30_ba_y_l                            \n",
      "1.2x decrease               -1.0    -1.0\n",
      "1.2x increase                1.0     1.0\n",
      "1.4x decrease               -2.0    -2.0\n",
      "1.4x increase                2.0     2.0\n",
      "1.7x or greater decrease    -3.0    -3.0\n",
      "1.7x or greater increase     3.0     3.0\n",
      "Near normal                  0.0     0.0\n",
      "No Data                  -9999.0 -9999.0\n",
      "                             min     max\n",
      "bau30_ww_y_l                            \n",
      "1.2x decrease               -1.0    -1.0\n",
      "1.2x increase                1.0     1.0\n",
      "1.4x decrease               -2.0    -2.0\n",
      "1.4x increase                2.0     2.0\n",
      "1.7x or greater decrease    -3.0    -3.0\n",
      "1.7x or greater increase     3.0     3.0\n",
      "Near normal                  0.0     0.0\n",
      "No Data                  -9999.0 -9999.0\n",
      "                          min  max\n",
      "bau30_ws_y_l                      \n",
      "1.4x decrease            -1.0 -1.0\n",
      "1.4xincrease              1.0  1.0\n",
      "2.0x decrease            -2.0 -2.0\n",
      "2.0x increase             2.0  2.0\n",
      "2.8x or greater decrease -3.0 -3.0\n",
      "2.8x or greater increase  3.0  3.0\n",
      "Near normal               0.0  0.0\n",
      "                             min     max\n",
      "bau30_wd_y_l                            \n",
      "1.4x decrease               -1.0    -1.0\n",
      "1.4xincrease                 1.0     1.0\n",
      "2.0x decrease               -2.0    -2.0\n",
      "2.0x increase                2.0     2.0\n",
      "2.8x or greater decrease    -3.0    -3.0\n",
      "2.8x or greater increase     3.0     3.0\n",
      "Near normal                  0.0     0.0\n",
      "No Data                  -9999.0 -9999.0\n",
      "                          min  max\n",
      "bau30_iv_y_l                      \n",
      "1.1x decrease            -1.0 -1.0\n",
      "1.1x increase             1.0  1.0\n",
      "1.2x decrease            -2.0 -2.0\n",
      "1.2x increase             2.0  2.0\n",
      "1.3x or greater decrease -3.0 -3.0\n",
      "1.3x or greater increase  3.0  3.0\n",
      "Near normal               0.0  0.0\n",
      "                          min  max\n",
      "bau30_sv_y_l                      \n",
      "1.1x decrease            -1.0 -1.0\n",
      "1.1x increase             1.0  1.0\n",
      "1.2x decrease            -2.0 -2.0\n",
      "1.2x increase             2.0  2.0\n",
      "1.3x or greater decrease -3.0 -3.0\n",
      "1.3x or greater increase  3.0  3.0\n",
      "Near normal               0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "inds = ['ba', 'ww', 'ws', 'wd', 'iv', 'sv']\n",
    "years = ['30']\n",
    "scens = ['bau']\n",
    "\n",
    "for i in inds:\n",
    "    for s in scens:\n",
    "        for y in years:\n",
    "            la = '{}{}_{}_y_l'.format(s,y,i)\n",
    "            ra = '{}{}_{}_y_r'.format(s,y,i)\n",
    "            sc = '{}{}_{}_y_s'.format(s,y,i)\n",
    "            ca = '{}{}_{}_y_c'.format(s,y,i)\n",
    "            df = df_4f_final.groupby(la)[ca].agg(['min', 'max'])\n",
    "            print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_names = {'bau': 'business_as_usual', 'opt': 'optimistic', 'pes': 'pessimistic'}\n",
    "indicator_names = {'ww': 'water_demand', 'ba': 'water_supply', \n",
    "              'ws':'water_stress', 'wd':'water_depletion', \n",
    "              'sv':'seasonal_variability', 'iv':'interannual_variability'}\n",
    "type_names = {'x': 'future_value', 'y': 'change_from_baseline'}\n",
    "data_names = {'r': 'value', 'l': 'label'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep raw and label\n",
    "df_4ff = df_4f.loc[:, [x for x in df_4f.columns if ('_r' in x) | ('_l' in x)]]\n",
    "# Melt data so columns become rows\n",
    "df_melt = df_4ff.melt(ignore_index = False)\n",
    "# Split old column name into individual columns by \"_\"\n",
    "df_melt[[0, 1 , 2 , 3]] = df_melt['variable'].str.split('_',  expand=True)\n",
    "# Create scenario\n",
    "df_melt['scenario'] = df_melt[0].apply(lambda x: scenario_names.get(x[0:3]))\n",
    "df_melt['year'] = df_melt[0].apply(lambda x: '20'+str(x[3:]))\n",
    "df_melt['indicator'] = df_melt[1].apply(lambda x: indicator_names.get(x))\n",
    "df_melt['type'] = df_melt[2].apply(lambda x: type_names.get(x))\n",
    "df_melt['data'] = df_melt[3].apply(lambda x: data_names.get(x))\n",
    "\n",
    "# Pivot data so raw and labels are side-by-side again\n",
    "df_pv = pd.pivot(data = df_melt.reset_index(), values = 'value', \n",
    "               index = ['pfaf_id', 'indicator', 'year', 'scenario', 'type'], columns = ['data']).reset_index()\n",
    "# Save in original order\n",
    "df_final = df_pv.set_index(['pfaf_id']).filter(['indicator', 'value', 'label', 'year', 'scenario', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(cartoPATH('future'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE CATEGORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = ['ws', 'wd', 'iv', 'sv']\n",
    "years = ['30', '50', '80']\n",
    "scens = ['bau', 'opt', 'pes']\n",
    "\n",
    "cats = ['x']\n",
    "tablesx = []\n",
    "for i in inds:\n",
    "    for s in scens:\n",
    "        for y in years:\n",
    "            for c in cats:\n",
    "                la = '{}{}_{}_{}_l'.format(s,y,i,c)\n",
    "                ca = '{}{}_{}_{}_c'.format(s,y,i,c)\n",
    "                df_f = df_4f_final.reset_index()\n",
    "                df = df_f.filter([la, ca]).drop_duplicates().sort_values(by = ca)\n",
    "                tablesx.append(df.reset_index().drop(['index'], axis = 1))\n",
    "            \n",
    "cats = ['y']\n",
    "tablesy = []\n",
    "for i in inds:\n",
    "    for s in scens:\n",
    "        for y in years:\n",
    "            for c in cats:\n",
    "                la = '{}{}_{}_{}_l'.format(s,y,i,c)\n",
    "                ca = '{}{}_{}_{}_c'.format(s,y,i,c)\n",
    "                df_f = df_4f_final.reset_index()\n",
    "                df = df_f.filter([la, ca]).drop_duplicates().sort_values(by = ca)\n",
    "                tablesy.append(df.reset_index().drop(['index'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tables = pd.concat(tablesx, axis = 1 )\n",
    "y_tables = pd.concat(tablesy, axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thresholds = pd.concat([x_tables, y_tables], axis = 1)\n",
    "df_thresholds.to_csv(futPATH('categories', 'csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bau30_ws_x_r', 'bau30_ws_x_s', 'bau30_ws_x_c', 'bau30_ws_x_l',\n",
       "       'bau30_ws_x_u', 'bau50_ws_x_r', 'bau50_ws_x_s', 'bau50_ws_x_c',\n",
       "       'bau50_ws_x_l', 'bau50_ws_x_u',\n",
       "       ...\n",
       "       'pes80_wd_y_r', 'pes80_wd_y_s', 'pes80_ws_y_c', 'pes80_ws_y_l',\n",
       "       'pes80_ws_y_r', 'pes80_ws_y_s', 'pes80_ww_y_c', 'pes80_ww_y_l',\n",
       "       'pes80_ww_y_r', 'pes80_ww_y_s'],\n",
       "      dtype='object', length=396)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_4f_final.columns"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "indicator_table_v2",
   "notebookOrigID": 2299896979574143,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
