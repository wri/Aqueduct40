{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35e3a216",
   "metadata": {},
   "source": [
    "# Step 5b: Aggregated Aqueduct data by administrative unit (state and country)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63bce95",
   "metadata": {},
   "source": [
    "1. Read in basin-state withdrawal data\n",
    "2. Read in basin Aqueduct indicators data\n",
    "3. Weight Aqueduct score by polygon's fraction of total admin unit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7731ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604e3c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS!\n",
    "# Country\n",
    "crtyROOT = r'\\Projections\\Final_Data\\Data\\Aqueduct40\\step5_country_rankings'\n",
    "# Final withdrawal data\n",
    "wwbcPATH = os.path.join(crtyROOT, \"withdrawals_basin-states-bias_corrected.csv\")  \n",
    "# Aqueduct indicators\n",
    "indPATH = r'\\Projections\\Final_Data\\Data\\Aqueduct40\\step3_calculate_indicators\\working'\n",
    "wsPATH = os.path.join(indPATH, 'Aqueduct40_indicators_annual-exploded-additive.csv')\n",
    "rootPATH = r'\\Projections\\Final_Data\\Data'\n",
    "aq3PATH = os.path.join(rootPATH, 'Aqueduct30', 'published', 'aqueduct-30-country-rankings.xlsx')\n",
    "aq3cartoPATH = os.path.join(rootPATH, 'Aqueduct30', 'published', 'aqueduct_results_v01_{}_v06_CARTO.csv').format\n",
    "# SAVE LOCATIONS\n",
    "finalPATH = os.path.join(crtyROOT, 'final', 'y2023m07d05_sk_Aqueduct40_country-state.xlsx')\n",
    "cartoPATH = os.path.join(rootPATH, 'Aqueduct40', 'step4_final_data_download', 'carto', 'aqueduct_results_{}_2023.csv').format\n",
    "workingPATH = os.path.join(crtyROOT, \"Aqueduct40_rankings_{}_working.csv\").format                   \n",
    "                         \n",
    "\n",
    "scenFolders = ['ssp126',\n",
    "               'ssp370',\n",
    "               'ssp585']\n",
    "gcmFolders = ['gfdl-esm4',\n",
    "              'ipsl-cm6a-lr',\n",
    "              'mpi-esm1-2-hr',\n",
    "              'mri-esm2-0',\n",
    "              'ukesm1-0-ll']\n",
    "\n",
    "ws_labels = {\n",
    "#     -5 : 'Zero Supply',\n",
    "    -9999 : 'No Data',\n",
    "    -1: 'Arid and Low Water Use',\n",
    "    0: 'Low (<10%)',\n",
    "    1: 'Low - Medium (10-20%)',\n",
    "    2: 'Medium - High (20-40%)',\n",
    "    3: 'High (40-80%)', \n",
    "    4: 'Extremely High (>80%)'\n",
    "}\n",
    "wd_labels = {\n",
    "#     -5 : 'Zero Supply',\n",
    "    -9999 : 'No Data',\n",
    "    -1: 'Arid and Low Water Use',\n",
    "    0: 'Low (<5%)',\n",
    "    1: 'Low - Medium (5-25%)',\n",
    "    2: 'Medium - High (25-50%)',\n",
    "    3: 'High (50-75%)', \n",
    "    4: 'Extremely High (>75%)'\n",
    "}\n",
    "\n",
    "iv_labels = {\n",
    "#     -5 : 'Zero Supply',\n",
    "    -9999 : 'No Data',\n",
    "    0: 'Low (<0.25)',\n",
    "    1: 'Low - Medium (0.25-0.50)',\n",
    "    2: 'Medium - High (0.50-0.75)',\n",
    "    3: 'High (0.75-1.00)', \n",
    "    4: 'Extremely High (>1.00)'\n",
    "}\n",
    "\n",
    "ind_names = {'ws_s': 'bws', 'wd_s': 'bwd', 'iv_s': 'iav'}\n",
    "\n",
    "# Function to create category value from score for all indicators\n",
    "def category(score, df_in):\n",
    "    cat = 'cat'\n",
    "    df_cat = np.floor(df_in[score]).to_frame(name = cat)\n",
    "    df_cat[cat] = np.where(df_cat[cat] == 5, 4, df_cat[cat])\n",
    "    return df_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c824d7",
   "metadata": {},
   "source": [
    "# 1. Read in final withdrawal data\n",
    "\n",
    "The values represent million cubic meters of annual demand per sector per milestone year. Irrigation data is the only sector that is calculated from PCR-GLOBWB model ouputs (because its's a function of climate). Demand and industry are projected by SSP, and are the same across the 5 GCMs per scenarion. Livestock withdrawal data ends in 2019. All future values equal the 2019 value. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b13c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in final demand data\n",
    "df_ww = pd.read_csv(wwbcPATH, index_col = ['basin_stat', 'year'])\n",
    "df_wwm = df_ww.melt(ignore_index = False)\n",
    "df_wwm[['sector', 'gcm', 'ssp']] = df_wwm['variable'].str.split('_',  expand=True)\n",
    "df_wwm['scen'] = df_wwm['gcm']  + \"_\" + df_wwm['ssp']\n",
    "# Clean data. \n",
    "# -- Remove 2014 rows\n",
    "df_wwm.reset_index(inplace = True)\n",
    "df_wwm = df_wwm.loc[df_wwm['year'] > 2014]\n",
    "# -- Rename baseline \n",
    "df_wwm.loc[df_wwm['scen'] == 'gswp3-w5e5_historical', 'scen'] = 'baseline_hist'\n",
    "# -- Drop null rows based on year-ssp combo\n",
    "df_wwm = df_wwm.loc[~((df_wwm.year.isin([2030, 2050, 2080])) & (df_wwm.ssp == 'historical'))]\n",
    "df_wwm =  df_wwm.loc[~((df_wwm.year.isin([2019])) & (df_wwm.ssp != 'historical'))]\n",
    "# -- Add IDs for catchcments and admins\n",
    "df_wwm[['pfaf_id', 'gid_1']] = df_wwm['basin_stat'].str.split('-',  expand=True)\n",
    "df_wwm['gid_0'] = df_wwm['gid_1'].apply(lambda x: x[0:3])\n",
    "# Set values as integers\n",
    "df_wwm['year'] = df_wwm['year'].astype(int)\n",
    "df_wwm['pfaf_id'] = df_wwm['pfaf_id'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb74a7",
   "metadata": {},
   "source": [
    "# 2. Read in Aqueduct 4.0 indicator data\n",
    "\n",
    "This data represents 1 step before the \"published\" data. It has a water stress score for each GCM/scenario, rather than the median. Data is in its \"exploded\" form, meaning each row is a unique combo of catchment, year, scen, and gcm, while the columns are Aqueduct values like raw, score, cat, label per indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f50896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Aqueduct indicator data\n",
    "df_ws = pd.read_csv(wsPATH, index_col = ['pfaf_id', 'period', 'scen'])\n",
    "# Keep water stress, water depletion, and interannual\n",
    "indicators = ['ws_s', 'wd_s', 'iv_s']\n",
    "df_wsf = df_ws.filter(indicators).reset_index()\n",
    "# Clean index. Make sure things are integers, and the naming is consistent with withdrawal data\n",
    "df_wsf.rename(columns = {'period': 'year'}, inplace = True)\n",
    "df_wsf['year'] = df_wsf['year'].astype(int)\n",
    "df_wsf['pfaf_id'] = df_wsf['pfaf_id'].astype(int)\n",
    "df_wsf.set_index(['pfaf_id', 'year', 'scen'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ffbd34",
   "metadata": {},
   "source": [
    "# 3. Perform weighted aggregation\n",
    "We can do this for any indicator. I've selected water stress, water depletion, and interannual variability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca832a8",
   "metadata": {},
   "source": [
    "## Find No Datas\n",
    "We identified which countries do not have enough PCR-GLOBWB data in Aqueduct 3.0 Mask new data using these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde49595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries: 25\n",
      "Provinces: 280\n"
     ]
    }
   ],
   "source": [
    "# Read in Aqueduct 3.0 Carto Data\n",
    "df_3ad0 = pd.read_csv(aq3cartoPATH('country')) \n",
    "df_3ad1 = pd.read_csv(aq3cartoPATH('province')) \n",
    "df_3ad1c = df_3ad1.drop_duplicates(subset = ['gid_1', 'indicator_name', 'weight'], keep='first')\n",
    "\n",
    "# Create lists of no data\n",
    "no_data_0 = df_3ad0.loc[(df_3ad0.indicator_name == 'bws') & (df_3ad0.weight == 'Tot') & (df_3ad0.score == -9999), 'gid_0'].tolist()\n",
    "no_data_1 = df_3ad1c.loc[(df_3ad1c.indicator_name == 'bws') & (df_3ad1c.weight == 'Tot') & (df_3ad1c.score == -9999), 'gid_1'].tolist()\n",
    "\n",
    "print('Countries: {}'.format(len(no_data_0)))\n",
    "print('Provinces: {}'.format(len(no_data_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51baeda5",
   "metadata": {},
   "source": [
    "## Aggregate by admin unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f8f6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_aggregation(gid, selected_indicators, no_data):\n",
    "    print(gid)\n",
    "    # 1. Sum withdrawal by country and state\n",
    "    df_tot = df_wwm.groupby([gid, 'year', 'scen', 'sector'])[['value']].sum()\n",
    "    df_tot.columns = ['tot']\n",
    "    df_tot['tot_w'] = df_tot['tot']\n",
    "    # 2. Create weights. Merge basin-state withdrawal water stress data with administrative total withdrawal\n",
    "    df_weight = pd.merge(df_wwm, df_wsf, how = 'inner', left_on = ['pfaf_id', 'year', 'scen'], right_index = True)\n",
    "    df_weight = pd.merge(df_weight, df_tot, how = 'left', left_on = [gid, 'year', 'scen', 'sector'], right_index = True)\n",
    "    # 3. Find % of geometry withdrawal to total of country or state\n",
    "    df_weight['wght'] = df_weight['value'].divide(df_weight['tot'])\n",
    "    # 4. Multiple each indicator by the %\n",
    "    for ind in selected_indicators:\n",
    "        print(\"create weight for:\", ind)\n",
    "        df_weight[ind + '_w'] = df_weight[ind].multiply(df_weight['wght'])\n",
    "\n",
    "    # 5. Finalize weighted sum for country and province\n",
    "    weight_cols = [x + \"_w\" for x in selected_indicators] + ['tot_w']\n",
    "    df_wgt = df_weight.groupby([gid, 'year', 'scen', 'sector'])[weight_cols].sum().reset_index()\n",
    "    # Take a non-weighted average\n",
    "    one_cols = selected_indicators + ['tot']\n",
    "    df_one = df_weight.groupby([gid, 'year', 'scen', 'sector'])[one_cols].mean().reset_index()\n",
    "\n",
    "    # 6. Now, find the median per SSP for future years\n",
    "    # -- First, define SSP\n",
    "    df_wgt['ssp'] = df_wgt['scen'].apply(lambda x: x.split(\"_\")[1])\n",
    "    df_one['ssp'] = df_one['scen'].apply(lambda x: x.split(\"_\")[1])\n",
    "    # # -- Second, find median\n",
    "    df_wgt_med = df_wgt.groupby([gid, 'year', 'ssp', 'sector'])[weight_cols].median().reset_index()\n",
    "    df_one_med = df_one.groupby([gid, 'year', 'ssp', 'sector'])[one_cols].median()\n",
    "    df_one_med = df_one_med.add_suffix(\"_w\").reset_index()\n",
    "\n",
    "    # 7. Clean the data frames\n",
    "    # -- For the non-weighted data, only keep the average from the total sum\n",
    "    df_one_med = df_one_med.loc[df_one_med['sector'] == 'gtotww']\n",
    "\n",
    "    # --Turn Sector column into weight column for both dataframes\n",
    "    df_wgt_med['weight'] = df_wgt_med['sector'].apply(lambda x: x[1:4].title())\n",
    "    df_one_med['weight'] = 'One'\n",
    "\n",
    "    # 8. Merge weighted with non-weighted\n",
    "    df_avgs = pd.concat([df_wgt_med, df_one_med], axis = 0)\n",
    "    \n",
    "    # Drop data set as no data if no data in Aqueduct 3.0\n",
    "    df_avg_valid = df_avgs.loc[~df_avgs[gid].isin(no_data)]\n",
    "    # 9. Find ranking\n",
    "    # -- First, create blank rank column for each indicator\n",
    "    for ind in selected_indicators:\n",
    "        print(\"create rank for:\", ind)\n",
    "        df_avg_valid[ind + '_rank'] = np.nan\n",
    "    # --Next, loop through each indicator, sector, year, and ssp and define rank for each\n",
    "    for ind in selected_indicators:\n",
    "#         print(i) \n",
    "        for w in ['Tot', 'Dom', 'Ind', 'Liv', 'Irr', 'One']:\n",
    "#             print(w)\n",
    "            for y in [2019, 2030, 2050, 2080]:\n",
    "                for s in ['hist', 'ssp126', 'ssp370', 'ssp585']:\n",
    "                    if y == 2019 and s != 'hist':\n",
    "                        continue\n",
    "                    elif y != 2019 and s == 'hist':\n",
    "                        continue\n",
    "#                     print(y, s)\n",
    "                    df_avg_valid.loc[(df_avg_valid['weight'] == w) \n",
    "                                & (df_avg_valid['year'] == y)\n",
    "                                & (df_avg_valid['ssp'] == s), ind + '_rank'] = df_avg_valid.loc[(df_avg_valid['weight'] == w) \n",
    "                                                                                 & (df_avg_valid['year'] == y)\n",
    "                                                                                 & (df_avg_valid['ssp'] == s), ind + '_w'].rank(ascending = False)\n",
    "    # Set index\n",
    "    df_avg_valid.set_index([gid, 'year', 'ssp', 'weight'], inplace = True)\n",
    "    return df_avg_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "face7aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gid_0\n",
      "create weight for: ws_s\n",
      "create weight for: wd_s\n",
      "create weight for: iv_s\n",
      "create rank for: ws_s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samantha.Kuzma\\AppData\\Local\\Temp\\ipykernel_23780\\3466462613.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_avg_valid[ind + '_rank'] = np.nan\n",
      "C:\\Users\\Samantha.Kuzma\\AppData\\Local\\Temp\\ipykernel_23780\\3466462613.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_avg_valid[ind + '_rank'] = np.nan\n",
      "C:\\Users\\Samantha.Kuzma\\AppData\\Local\\Temp\\ipykernel_23780\\3466462613.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_avg_valid[ind + '_rank'] = np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create rank for: wd_s\n",
      "create rank for: iv_s\n",
      "gid_1\n",
      "create weight for: ws_s\n",
      "create weight for: wd_s\n",
      "create weight for: iv_s\n",
      "create rank for: ws_s\n",
      "create rank for: wd_s\n",
      "create rank for: iv_s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samantha.Kuzma\\AppData\\Local\\Temp\\ipykernel_23780\\3466462613.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_avg_valid[ind + '_rank'] = np.nan\n",
      "C:\\Users\\Samantha.Kuzma\\AppData\\Local\\Temp\\ipykernel_23780\\3466462613.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_avg_valid[ind + '_rank'] = np.nan\n",
      "C:\\Users\\Samantha.Kuzma\\AppData\\Local\\Temp\\ipykernel_23780\\3466462613.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_avg_valid[ind + '_rank'] = np.nan\n"
     ]
    }
   ],
   "source": [
    "df_0 = perform_aggregation(gid = 'gid_0', selected_indicators = indicators, no_data = no_data_0)\n",
    "df_1 = perform_aggregation(gid = 'gid_1', selected_indicators = indicators, no_data = no_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd2c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0.to_csv(workingPATH('countries'))\n",
    "df_1.to_csv(workingPATH('provinces'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38d7087",
   "metadata": {},
   "source": [
    "# 4. Create final download and Carto table\n",
    "We will use baseline water stress, drought risk and riverine flood risk in the final public document. Drought risk and riverine flood risk can be taken from Aqueduct 3.0 because we did not make any changes to the SCORE values (the drough RAW values were fixed. In Aq 3.0, they equaled the score by mistake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad5393dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp_dict = {'ssp126': 'opt', 'ssp370': 'bau', 'ssp585': 'pes'}\n",
    "\n",
    "def clean_indicator(df_in, i, labels, gid):\n",
    "    # filter data\n",
    "    df_f = df_in.filter([i + \"_w\", i + \"_rank\"] + ['tot_w'])\n",
    "    # rename indicator score, rank, and sum of weights\n",
    "    df_f.columns = ['score', 'score_ranked', 'sum_weights']\n",
    "    # calculate sum weight per indicator\n",
    "    df_f['sum_weighted_indicator'] = df_f['sum_weights'].multiply(df_f['score'])\n",
    "    # add clean indicator name\n",
    "    df_f['indicator_name'] = ind_names.get(i)\n",
    "    # add category and label\n",
    "    df_f['cat'] = category(score = 'score', df_in = df_f)['cat']\n",
    "    df_f['label'] = df_f['cat'].map(labels)\n",
    "    # redo Index\n",
    "    df_f.reset_index(inplace = True)\n",
    "    df_f['scenario'] = df_f['ssp'].apply(lambda x: ssp_dict.get(x))\n",
    "    df_f.drop(['ssp'], axis = 1, inplace = True)\n",
    "    df_f.rename(columns = {'year':'period'}, inplace = True)\n",
    "    df_f.set_index([gid, 'period', 'scenario', 'indicator_name', 'weight'], inplace = True)\n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a7b68",
   "metadata": {},
   "source": [
    "## BASELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493884e",
   "metadata": {},
   "source": [
    "### COUNTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50abf2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of OG: 2456\n",
      "Original Max Drought Score: 0.831943067\n",
      "New Max Drought Score: 4.159715335\n",
      "length of new: 2456\n"
     ]
    }
   ],
   "source": [
    "# Read in 3.0 country data. Create list of accepted countries\n",
    "# IF USING DOWNLOADABLE DATA AS STARTING PLACE\n",
    "# df_3ad0 = pd.read_excel(aq3PATH, sheet_name = 'results country')\n",
    "# df_3ad0.rename(columns = {'iso_a3': 'gid_0'}, inplace = True)\n",
    "# df_3ad0.set_index(['gid_0', 'indicator_name', 'weight'], inplace = True)\n",
    "\n",
    "# IF USING CARTO AS STARTING PLACE\n",
    "df_3ad0 = pd.read_csv(aq3cartoPATH('country'), index_col = ['gid_0', 'indicator_name', 'weight']) \n",
    "print('length of OG:', len(df_3ad0))\n",
    "\n",
    "# Multiply old Drought score by 5\n",
    "print('Original Max Drought Score: {}'.format(df_3ad0.loc[df_3ad0.index.get_level_values('indicator_name') == 'drr', 'score'].max()))\n",
    "df_3ad0.loc[df_3ad0.index.get_level_values('indicator_name') == 'drr', 'score'] = df_3ad0.loc[df_3ad0.index.get_level_values('indicator_name') == 'drr', 'score'] * 5\n",
    "# Reset -9999 * 5 to -9999\n",
    "df_3ad0['score'] = df_3ad0['score'].mask(df_3ad0['score'] == -49995, -9999)\n",
    "print('New Max Drought Score: {}'.format(df_3ad0.loc[df_3ad0.index.get_level_values('indicator_name') == 'drr', 'score'].max()))\n",
    "\n",
    "# # Set BWS data to NaN \n",
    "data_cols = ['score', 'score_ranked', 'cat', 'label', 'sum_weights', 'sum_weighted_indicator',]\n",
    "df_3ad0.loc[df_3ad0.index.get_level_values('indicator_name') == 'bws', data_cols] = np.nan\n",
    "\n",
    "# # # Create clean version of baseline water stress\n",
    "df_0ws = clean_indicator(df_in = df_0, i = 'ws_s', labels = ws_labels, gid = 'gid_0')\n",
    "# # # Create a version of the baseline only\n",
    "df_0wsb = df_0ws.loc[df_0ws.index.get_level_values('period') == 2019]\n",
    "df_0wsb = df_0wsb.droplevel(['period', 'scenario'])\n",
    "# # FIll WS data with new results\n",
    "df_4ad0 = df_3ad0.fillna(df_0wsb)\n",
    "\n",
    "# Drop old columns\n",
    "df_4ad0.drop(['count_valid', 'fraction_valid', 'primary', 'valid_hybas6'], axis = 1, inplace = True)\n",
    "# Set no data = -9999\n",
    "df_4ad0.loc[(df_4ad0.index.get_level_values('indicator_name') == 'bws') & (df_4ad0['score'].isna()), data_cols] = -9999\n",
    "df_4ad0.loc[(df_4ad0.index.get_level_values('indicator_name') == 'bws') & (df_4ad0['score'] == -9999), 'label'] = 'NoData'\n",
    "print('length of new:', len(df_4ad0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4145d83",
   "metadata": {},
   "source": [
    "### PROVINCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b8c1284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of OG: 42771\n",
      "Original Max Drought Score: 0.970316846\n",
      "New Max Drought Score: 4.85158423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samantha.Kuzma\\AppData\\Local\\Temp\\ipykernel_23780\\2016594663.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_3ad1c['score'] = df_3ad1c['score'].mask(df_3ad1c['score'] == -49995, -9999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of new: 42771\n"
     ]
    }
   ],
   "source": [
    "# Read in 3.0 country data. Create list of accepted countries\n",
    "# IF USING DOWNLOADABLE DATA AS STARTING PLACE\n",
    "# df_3ad1 = pd.read_excel(aq3PATH, sheet_name = 'results province')\n",
    "# df_3ad1.rename(columns = {'iso_a3': 'gid_0'}, inplace = True)\n",
    "# df_3ad1c = df_3ad1.drop_duplicates(subset = ['gid_1', 'indicator_name', 'weight'], keep='first')\n",
    "# df_3ad1c.set_index(['gid_1', 'indicator_name', 'weight'], inplace = True)\n",
    "\n",
    "# IF USING CARTO AS STARTING PLACE\n",
    "df_3ad1 = pd.read_csv(aq3cartoPATH('province')) \n",
    "df_3ad1c = df_3ad1.drop_duplicates(subset = ['gid_1', 'indicator_name', 'weight'], keep='first')\n",
    "df_3ad1c.set_index(['gid_1', 'indicator_name', 'weight'], inplace = True)\n",
    "print('length of OG:', len(df_3ad1c))\n",
    "\n",
    "# Multiply old Drought score by 5\n",
    "print('Original Max Drought Score: {}'.format(df_3ad1c.loc[df_3ad1c.index.get_level_values('indicator_name') == 'drr', 'score'].max()))\n",
    "df_3ad1c.loc[df_3ad1c.index.get_level_values('indicator_name') == 'drr', 'score'] = df_3ad1c.loc[df_3ad1c.index.get_level_values('indicator_name') == 'drr', 'score'] * 5\n",
    "# Reset -9999 * 5 to -9999\n",
    "df_3ad1c['score'] = df_3ad1c['score'].mask(df_3ad1c['score'] == -49995, -9999)\n",
    "print('New Max Drought Score: {}'.format(df_3ad1c.loc[df_3ad1c.index.get_level_values('indicator_name') == 'drr', 'score'].max()))\n",
    "\n",
    "\n",
    "# # Set BWS data to NaN \n",
    "data_cols = ['score', 'score_ranked', 'cat', 'label', 'sum_weights', 'sum_weighted_indicator',]\n",
    "df_3ad1c.loc[df_3ad1c.index.get_level_values('indicator_name') == 'bws', data_cols] = np.nan\n",
    "\n",
    "# # # Create clean version of baseline water stress\n",
    "df_1ws = clean_indicator(df_in = df_1, i = 'ws_s', labels = ws_labels, gid = 'gid_1')\n",
    "# # # Create a version of the baseline only\n",
    "df_1wsb = df_1ws.loc[(df_1ws.index.get_level_values('period') == 2019)]\n",
    "df_1wsb = df_1wsb.droplevel(['period', 'scenario'])\n",
    "# # FIll WS data with new results\n",
    "df_4ad1 = df_3ad1c.fillna(df_1wsb)\n",
    "\n",
    "# Drop old columns\n",
    "drop_cols = ['count_valid', 'fraction_valid', 'primary', 'valid_hybas6']\n",
    "df_4ad1.drop(drop_cols, axis = 1, inplace = True)\n",
    "# Set no data = -9999\n",
    "df_4ad1.loc[(df_4ad1.index.get_level_values('indicator_name') == 'bws') & (df_4ad1['score'].isna()), data_cols] = -9999\n",
    "df_4ad1.loc[(df_4ad1.index.get_level_values('indicator_name') == 'bws') & (df_4ad1['score'] == -9999), 'label'] = 'NoData'\n",
    "print('length of new:', len(df_4ad1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f0ac0",
   "metadata": {},
   "source": [
    "## FUTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc16fa6",
   "metadata": {},
   "source": [
    "### COUNTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8f2ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean water stress data\n",
    "df_0ws = clean_indicator(df_in = df_0, i = 'ws_s', labels = ws_labels, gid = 'gid_0')\n",
    "# # # Create a version of the future only\n",
    "df_0wsf = df_0ws.loc[df_0ws.index.get_level_values('period') != 2019]\n",
    "df_0wsf = df_0wsf.reset_index().set_index(['gid_0'])\n",
    "# Read in 3.0 country data. Create list of accepted countries\n",
    "df_3ad0 = pd.read_excel(aq3PATH, sheet_name = 'results country')\n",
    "df_3ad0.rename(columns = {'iso_a3': 'gid_0'}, inplace = True)\n",
    "# Create list of approved names from Aq 3.0 data\n",
    "df_names = df_3ad0.loc[(df_3ad0['indicator_name'] == 'bws') & (df_3ad0['weight'] == 'Tot')]\n",
    "df_names.set_index(['gid_0'], inplace = True)\n",
    "df_names = df_names.filter(['iso_n3', 'name_0', 'un_region', 'wb_region'])\n",
    "# Add name columns to future results\n",
    "df_4ad0f = pd.merge(df_0wsf, df_names, how = 'inner', left_index = True, right_index = True)\n",
    "df_4ad0f.reset_index().set_index(['gid_0', 'scenario', 'period', 'indicator_name', 'weight'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5773b6c6",
   "metadata": {},
   "source": [
    "### PROVINCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96631718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean water stress data\n",
    "df_1ws = clean_indicator(df_in = df_1, i = 'ws_s', labels = ws_labels, gid = 'gid_1')\n",
    "# # # Create a version of the baseline only\n",
    "df_1wsf = df_1ws.loc[df_1ws.index.get_level_values('period') != 2019]\n",
    "df_1wsf = df_1wsf.reset_index().set_index(['gid_1'])\n",
    "# Read in 3.0 country data. Create list of accepted countries\n",
    "df_3ad1 = pd.read_excel(aq3PATH, sheet_name = 'results province')\n",
    "df_3ad1.rename(columns = {'iso_a3': 'gid_0'}, inplace = True)\n",
    "df_names = df_3ad1.loc[(df_3ad1['indicator_name'] == 'bws') & (df_3ad1['weight'] == 'Tot')]\n",
    "df_names.set_index(['gid_1'], inplace = True)\n",
    "df_names = df_names.filter(['iso_n3', 'gid_0', 'name_0', 'name_1', 'un_region', 'wb_region'])\n",
    "df_4ad1f = pd.merge(df_1wsf, df_names, how = 'inner', left_index = True, right_index = True)\n",
    "df_4ad1f.reset_index().set_index(['gid_1', 'scenario', 'period', 'indicator_name', 'weight'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53436b30",
   "metadata": {},
   "source": [
    "# SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8c220e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4ad0.to_csv(cartoPATH('country'))\n",
    "df_4ad1.to_csv(cartoPATH('province'))\n",
    "df_4ad0f.to_csv(cartoPATH('country_future'))\n",
    "df_4ad1f.to_csv(cartoPATH('province_future'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
